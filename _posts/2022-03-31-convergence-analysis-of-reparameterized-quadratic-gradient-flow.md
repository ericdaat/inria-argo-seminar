---
layout: post
title: Convergence analysis of reparameterized quadratic gradient flow
speaker: David Robin
speaker_url:
speaker_institution:
date: 2022-03-31
---

The quest for understanding of convergence of deep learning methods is paved with puzzling properties, from the apparent vanishing of the bias-variance tradeoff, to the ambient non-convexity which renders all usual analysis techniques useless. The field of "deep learning" is so broad that essentially any machine-computable function can be interpreted as a neural network, therefore it seems reasonable to expect that a theory explaining the success of deep learning should at least explain the behavior of the trivial cases such as linear regressions (aka. single-layer networks) and their reparameterizations. To understand what this entails, we present an in-depth convergence analysis of the (simple) case of linear regressions reparameterized as mirror flows, complete with characterizations of minima and convergence speeds guarantees. We conclude by giving intuitions on how these reparameterizations can be leveraged to manipulate both the limit point of gradient flows and their speed of convergence.
